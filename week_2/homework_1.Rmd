---
title: "Week 2 Homework"
author: "Pablo Bello"
date: '2023-01-18'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
###--- Libraries
library(tidyverse)
library(ggpubr)
library(brms)


###--- Theme
theme_set(theme_pubr(border = TRUE))

###--- Data
tbl <- 
  read_csv("week_2/twitchdata-update.csv") |> 
  janitor::clean_names()
```


### Question 1

We will begin with an easy question. An almost obvious question. We are going to examine whether the number of followers a streamer has is predictive of the average viewers they get. Following what the chapter told us, let's look at the raw data. Show me the average_viewers and the followers for five random streamers. What do you notice?

```{r}
tbl |> 
  slice_sample(n = 5) |> 
  select(average_viewers,followers) |> 
  arrange(followers)

```

**The rank order correlation is good.**


Now, let's summarize these two variables. An alternative way you get a summary of your variables of interest is by running summary() on them. Select our two variables of interest and run summary(). Describe the results in a few words. Does anything capture your attention?

```{r}
tbl |> 
  select(average_viewers,followers) |> 
  summary()

```

**Medians are smaller than means so both distributions are right-skewed (average viewers more so as mean is almost on the 3rd quartile) as it is common with unconstrained popularity measures.**

Okay, lastly - but perhaps most importantly - lets visualize the data. Make a scatterplot with our two variables of interest.

```{r}
(p1 <- 
  tbl |> 
  mutate(followers = followers/1e6,
         average_viewers = average_viewers/1e3) |> 
  ggplot(aes(average_viewers,followers)) +
  geom_point(alpha = .3) +
  labs(x = "Average Viewers (Thousands)",
       y = "Number of Followers (Millions)"))

```

What do you notice?

**Exactly what it says below.**

Right away, you should notice that the data is packed into a small part of the Cartesian plane. Why? Because we have an uneven distribution - a few channels with a lot of followers and a lot of average viewers. So what should we do? We can transform the data. Remember the scale_x_log10 trick we learned in the last book? Let's apply it. Make the same plot but adding scale_x_log10 and scale_y_log10. What do you see now? How does the relationship look like?

```{r}
p1 +
  scale_x_log10() +
  scale_y_log10()
```


**Now it looks like a pretty clear linear relationship.**

Hopefully you have learned something important here: often the relationship between two variables is not immediately obvious and we need to do some transformations of the data to uncover it. Let's add those transformed variables to our dataset.

```{r}
tbl <- 
  tbl |> 
  mutate(log_viewers = log10(average_viewers), 
         log_followers = log10(followers))
```


### Question 2

Let's actually run a regression. Using lm() fit a model where you predict the logarithm of average viewers (log_viewers) using the logarithm of followes (log_followers). Save the results to an object called fit1.


```{r}
###--- With BRMS
#hist(tbl$log_followers)
#hist(tbl$log_viewers)


###--- Priors
priors <- c(prior(normal(0, 1), class = b, coef = log_followers),
           prior(exponential(1), class = sigma))

###--- Formula
formula <- bf(log_viewers ~ log_followers)

###--- Model
fit_1 <- 
  brms::brm(data = tbl, 
      family = gaussian,
      formula,
      prior = priors,
      chains = 4, 
      cores = 4,
      seed = 9,
      file = "fits/model_x")

print(fit_1)

#stancode(model)

```

I am going to show you another way of getting a summary of your model. First, let's install the broom package. After, run tidy() on your model object (fit1).

```{r}
library(broom.mixed)

broom.mixed::tidy(fit_1) |> 
  select(-c(effect,component,group))

```

Before I have you describe your results I have to tell you that when you transform your variables, interpretation is a bit different. In the situation we are in - where your outcome and explanatory variables have been logged - the coefficients are interpreted as percentage increases. For example, let's say we have a coefficient of .4 We would do the following:

$$ 1.1^{0.4} = 1.03886 $$ And we would interpret our coefficient like this:

A 10% increase in followers is associated with a 3.9% increase in the average number of viewers.

Now, it's your turn. Take the coefficient from your model and interpret it in this way.

$$(1.1^{0.59} - 1)*100 \approx 5.8$$ 
**A 10% increase in followers is associated to a 5.8% increase in the number of viewers.**

### Question 3

Okay, now let's look at our line of best fit and check the residuals. I am again going to introduce you to an incredibly useful tool from the broom package called augment. Run the following code:

```{r}
preds <- 
  broom.mixed::augment(fit_1) |> 
  janitor::clean_names()

glimpse(preds)
```

```{r}
preds %>% 
  ggplot(aes(x = log_followers, 
             y = log_viewers)) +
  geom_jitter(alpha = 0.4) + 
  geom_line(aes(x = log_followers, 
                y = fitted), 
            col = "orange") + 
  labs(subtitle = "Fitted Model and Raw Data", 
       title = "Followers & Average Viewership", 
       x = "log(followers)", 
       y = "log(viewers)")

```

Do you think our model describes the relationship well?

**Yes.**

Now, you fit a plot where log_followers is in the x-axis and .resid is in the y-axis.

```{r}

p2 <- 
  preds |> 
  ggplot(aes(x = log_followers, 
             y = resid)) +
  geom_point(alpha = 0.4) +
  labs(title = "Predictor and Residuals", 
       x = "log(followers)", 
       y = "Residuals")


ggExtra::ggMarginal(p2, type="density",margins = "y")

```

What do you see? Are there any big residuals? DO they happen often in a particular range of our x-variable? If so, we would have a problem: our model would systematically fail to predict part of our data.

**Some bigger residuals but no apparent pattern.**

### Question 4

Let's now look at regression using one categorical variable to predict one continuous variable. Here, I am interested in whether language predicts average_viewers. This would give us an indication of where the most popular twitch channels come from. I have a hunch that English streamers might be the most popular. Let's see.

First, describe our variables of interest as we did above. I am going to give you less guidance here. I want you to explore:

The raw data
Summaries of the variables
Plot the variables

```{r}
###--- The raw data 
tbl |> 
  select(language, average_viewers) |> 
  slice_sample(n = 5)
```


```{r}
###--- Summarise and Plot the variables
tbl |> 
  count(language) |> 
  mutate(language = fct_reorder(language,n,.desc = TRUE)) |> 
  ggplot(aes(language, n)) +
  geom_col() +
  labs(y = "Count",
       x = "Language",
       title = "Number of Channels by Language") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))


###--- 
tbl |> 
  group_by(language) |> 
  mutate(av_view = median(average_viewers)) |> 
  ungroup() |> 
  mutate(language = fct_reorder(language,av_view,.desc = TRUE)) |> 
  ggplot(aes(language,average_viewers)) +
  geom_boxplot() +
  scale_y_log10() +
  labs(x = "Language",
       y = "log10(Average Viewers)",
       title = "Channel Language and Number of Viewers") +
  theme(axis.text.x = element_text(angle = 45, vjust = 0.5))



```


**The most popular language is English, with a big difference over the rest (I could also summarise language in a count table but a plot is easier to read). In the second plot, there is a relationship between language and the number of viewers. Channels in Arabic have the highest median, Swedish ones the lowest.**

### Question 5

Now, we are ready to fit the model. Fit a linear regression where your outcome variable is average_viewers and your independent variable is language. Let me teach you another trick here. When your categorical variable has many categories it makes sense to establish your reference category outside of the model. This ensures that, when you are reading your coefficients, you know what you are comparing them to. Let's set English as our reference category.

```{r}

###--- Priors
priors <- c(prior(normal(1e3, 1e3), class = b),
            prior(exponential(1), class = sigma))

###--- Formula
formula <- bf(average_viewers ~ 0 + language)

###--- Model
fit_2 <- 
  brms::brm(data = tbl, 
      family = gaussian,
      formula,
      prior = priors,
      chains = 4, 
      cores = 4,
      seed = 9,
      file = "fits/model_z")

print(fit_2)

###--- Tidy model summary
coeffs_fit_2 <- 
  broom.mixed::tidy(fit_2) |> 
  janitor::clean_names() |> 
  select(-c(effect,component,group)) |> 
  mutate(term = str_remove(term,"language")) |> 
  arrange(desc(estimate))


fit_2_lm <- lm(average_viewers ~ language + 0, data = tbl)

coeffs_fit_2_lm <- 
  broom::tidy(fit_2_lm) |> 
  janitor::clean_names() |> 
  select(term,estimate, std_error) |> 
  mutate(term = str_remove(term,"language"),
        conf_low = estimate - 1.96*std_error,
        conf_high = estimate + 1.96*std_error
         ) |> 
  arrange(desc(estimate))



###--- Plot Estimates
bind_rows(bayes = coeffs_fit_2,
          lm = coeffs_fit_2_lm,.id = "model") |> 
  filter(str_detect(term, "sd_") == FALSE) |> 
  mutate(term = fct_reorder(term,estimate)) |> 
  ggplot(aes(estimate,term)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf_low, xmax = conf_high)) +
  facet_wrap(~ model) 

```



**Here I use the no intercept trick. Both bayesian and frequentist version are fitted. The plot compares the coefficients for both. In this case because I choose prior that are much lower than the coefficients and the **
